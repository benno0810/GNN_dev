{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'losses'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-9423d8ede144>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmatplotlib\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpyplot\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mplt\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnn\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mMSELoss\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 18\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mlosses\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mcompute_loss_multiclass\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     19\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'losses'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import math\n",
    "import dgl\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from dgl.data import citation_graph as citegrh\n",
    "from dgl.data import CoraBinary\n",
    "from dgl.data import CoraGraphDataset\n",
    "from dgl import DGLGraph\n",
    "import dgl.function as fn\n",
    "import networkx as nx\n",
    "import torch.nn.functional as F\n",
    "from dgl.data import RedditDataset, KarateClubDataset\n",
    "from dgl.nn import GraphConv\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import MSELoss\n",
    "from losses import compute_loss_multiclass\n",
    "\n",
    "\n",
    "class MyModel(th.nn.Module):\n",
    "    def __init__(self, g, dropout, n_features):\n",
    "        '''\n",
    "\n",
    "        :param g:\n",
    "        :param dropout:\n",
    "\n",
    "        c_hat = ReLU(f1*c+f2*(Q C)+b) = (nX1)\n",
    "        Q= nXn\n",
    "        C = nX1\n",
    "        Q*C = nX1\n",
    "        so dimmension of  input is [n,2], output [n,1], Linear layer  [2,1]\n",
    "        '''\n",
    "        super(MyModel, self).__init__()\n",
    "        self.g = g\n",
    "        self.layers = th.nn.ModuleList()\n",
    "        self.layers.append(th.nn.Linear(n_features, 2))\n",
    "        self.layers.append(th.nn.ReLU(inplace=True))\n",
    "        self.dropout = th.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, features):\n",
    "        h = features.float()\n",
    "        for i, layers in enumerate(self.layers):\n",
    "            if i != 0:\n",
    "                h = self.dropout(h)\n",
    "            h = layers(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class ModularityScore(th.nn.Module):\n",
    "    def __init__(self,n_classes,cuda):\n",
    "        super(ModularityScore, self).__init__()\n",
    "        ## define C as parameter\n",
    "        #self.params = th.nn.ParameterList([C])\n",
    "        self.cuda=cuda\n",
    "\n",
    "\n",
    "    def forward(self,C,Q):\n",
    "        # -tf.linalg.trace(tf.matmul(tf.matmul(tf.transpose(C),Q),C))\n",
    "        C=th.sigmoid(C)\n",
    "        Q=Q.float()\n",
    "        if self.cuda:\n",
    "            C=C.cuda()\n",
    "            Q=Q.cuda()\n",
    "        temp = th.matmul(th.matmul(C.t(), Q), C)\n",
    "        loss = -temp.trace()\n",
    "        return loss\n",
    "\n",
    "\n",
    "class GCN(th.nn.Module):\n",
    "    def __init__(self,\n",
    "                 g,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 n_classes,\n",
    "                 n_layers,\n",
    "                 activation,\n",
    "                 dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.g = g\n",
    "        self.layers = th.nn.ModuleList()\n",
    "        # input layer\n",
    "        self.layers.append(GraphConv(in_feats, n_hidden, activation=activation))\n",
    "        # output layer\n",
    "        for i in range(n_layers - 1):\n",
    "            self.layers.append(GraphConv(n_hidden, n_hidden, activation=activation))\n",
    "        # output layer\n",
    "        self.layers.append(GraphConv(n_hidden, n_classes))\n",
    "        self.dropout = th.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, features):\n",
    "        h = features\n",
    "        for i, layers in enumerate(self.layers):\n",
    "            if i != 0:\n",
    "                h = self.dropout(h)\n",
    "            h = layers(self.g, h)\n",
    "        return h\n",
    "\n",
    "\n",
    "def evaluate(model, features, labels, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "        correct = torch.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)\n",
    "\n",
    "\n",
    "def Q2(G1: dgl.DGLGraph):\n",
    "    # calculate matrix Q with diag set to 0\n",
    "    # A=np.array(nx.adjacency_matrix(G1).todense())\n",
    "    G1 = dgl.to_networkx(G1)\n",
    "    A = np.array(nx.adjacency_matrix(G1).todense())\n",
    "    T = A.sum(axis=(0, 1))\n",
    "    Q = A * 0\n",
    "    w_in = A.sum(axis=1)\n",
    "    w_out = w_in.reshape(w_in.shape[0], 1)\n",
    "    K = w_in * w_out / T\n",
    "    Q = (A - K) / T\n",
    "    # set Qii to zero for every i\n",
    "    for i in range(Q.shape[0]):\n",
    "        Q[i][i] = 0\n",
    "    return Q\n",
    "\n",
    "\n",
    "# a utility function to convert a scipy.coo_matrix to torch.SparseFloat\n",
    "def sparse2th(mat):\n",
    "    value = mat.data\n",
    "    indices = th.LongTensor([mat.row, mat.col])\n",
    "    # tensor = th.FloatTensor(th.from_numpy(value).float())\n",
    "    tensor = th.sparse.FloatTensor(indices, th.from_numpy(value).float(), mat.shape)\n",
    "    return tensor.to_dense()\n",
    "\n",
    "\n",
    "# network visualization utility function\n",
    "def visualize(labels, g):\n",
    "    pos = nx.spring_layout(g, seed=1)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.axis('off')\n",
    "    nx.draw_networkx(g, pos=pos, node_size=50, cmap=plt.get_cmap('coolwarm'),\n",
    "                     node_color=labels, edge_color='k',\n",
    "                     arrows=False, width=0.5, style='dotted', with_labels=False)\n",
    "\n",
    "def load_cora_binary():\n",
    "    data = CoraBinary()\n",
    "    g,features,labels=data[1]\n",
    "    n_edges=g.number_of_edges()\n",
    "    features=sparse2th(features)\n",
    "    labels=th.LongTensor(labels)\n",
    "    in_feats=features.shape[1]\n",
    "    n_classes=2\n",
    "    print(th.max(features))\n",
    "\n",
    "    return g,features,n_classes,in_feats,n_edges,labels\n",
    "\n",
    "def load_kara():\n",
    "    data =KarateClubDataset()\n",
    "    n_classes = data.num_classes\n",
    "    g = data[0]\n",
    "    n_edges=g.number_of_edges()\n",
    "    n=len(g.ndata['label'])\n",
    "    labels=g.ndata['label']\n",
    "    #construct features, train,val,test masks\n",
    "    g.ndata['feat']= th.eye(n)\n",
    "    in_feats=g.ndata['feat'].shape[1]\n",
    "    features=torch.FloatTensor(g.ndata['feat'])\n",
    "\n",
    "    return g,features,n_classes,in_feats,labels,n\n",
    "\n",
    "def load_cora():\n",
    "\n",
    "    data = citegrh.load_cora()\n",
    "    features = torch.FloatTensor(data.features)\n",
    "    labels = torch.LongTensor(data.labels)\n",
    "    train_mask = torch.BoolTensor(data.train_mask)\n",
    "    val_mask = torch.BoolTensor(data.val_mask)\n",
    "    test_mask = torch.BoolTensor(data.test_mask)\n",
    "    in_feats = features.shape[1]\n",
    "    n_classes = data.num_labels\n",
    "    n_edges = data.graph.number_of_edges()\n",
    "    g = DGLGraph(data.graph)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dropout = 0.5\n",
    "    gpu = 0\n",
    "    lr = 5e-2\n",
    "    n_epochs = 20000\n",
    "    n_hidden =32  # hidden node number for each layer\n",
    "    n_layers = 2  # number of layer\n",
    "    weight_decay = 5e-4  # weight decay not used here\n",
    "    self_loop = True  # check cycle in the network\n",
    "\n",
    "    # load cora_binary, train_masks,val_masks,test_masks are used for future accuracy comparement with supervised algorithm\n",
    "    g, features, n_classes, in_feats, n_edges,labels = load_cora_binary()\n",
    "    n = len(labels)\n",
    "    train_mask = [True] * n\n",
    "    train_mask=th.BoolTensor(train_mask)\n",
    "    val_mask = train_mask\n",
    "    test_mask = train_mask\n",
    "\n",
    "\n",
    "    #calculate matrix Q, initial community attachment C (with overlap)\n",
    "    Q = Q2(g)\n",
    "    # NOT OVERLAPING CASE\n",
    "    C_init = Q[0:2] * 0\n",
    "    C_init[0] = np.random.randint(2, size=(1, Q.shape[0]))\n",
    "    C_init[1] = 1 - C_init[0]\n",
    "    C = th.tensor(data=C_init.T, requires_grad=True)\n",
    "    C=C.float()\n",
    "    Q = th.from_numpy(Q)\n",
    "    Q=Q.float()\n",
    "    Q_C = th.matmul(Q,C)\n",
    "\n",
    "    if gpu < 0:\n",
    "        cuda = False\n",
    "    else:\n",
    "        cuda = True\n",
    "        torch.cuda.set_device(gpu)\n",
    "        features = features.cuda()\n",
    "        labels = labels.cuda()\n",
    "        g=g.to('cuda:0')\n",
    "        train_mask = train_mask.cuda()\n",
    "        val_mask = val_mask.cuda()\n",
    "        test_mask = test_mask.cuda()\n",
    "\n",
    "    degs = g.in_degrees().float()\n",
    "\n",
    "    model = GCN(g,\n",
    "            in_feats,\n",
    "            n_hidden,\n",
    "            n_classes,\n",
    "            n_layers,\n",
    "            F.relu,\n",
    "            dropout)\n",
    "\n",
    "    for p in model.parameters():\n",
    "        print(p)\n",
    "\n",
    "\n",
    "\n",
    "    # use Modularity score as object function, calculated by M=(C^T)QC\n",
    "    loss_fcn =ModularityScore(n_classes,cuda)\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    for p in loss_fcn.parameters():\n",
    "        print(p)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(),\n",
    "                                  lr=lr)\n",
    "\n",
    "    # train and evaluate (with modularity score and labels)\n",
    "    dur = []\n",
    "    M=[]\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        t0 = time.time()\n",
    "        C_hat = model(features)\n",
    "        loss = loss_fcn(C_hat,Q)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        dur.append(time.time() - t0)\n",
    "        if epoch % 100 == 0:\n",
    "            #record modularity\n",
    "            M.append(str(-loss.item()))\n",
    "            acc_1 = evaluate(model, features, labels, val_mask)\n",
    "            acc_2 = evaluate(model, features, 1 - labels, val_mask)\n",
    "            acc = max(acc_1, acc_2)\n",
    "            print(\"Epoch {} | Time(s) {} | Modularity {} | Accuracy {} | \"\n",
    "                  \"ETputs(KTEPS) {}\".format(epoch, np.mean(dur), -loss,\n",
    "                                                acc, n_edges / np.mean(dur) / 1000))\n",
    "\n",
    "    with open('modularity_history.txt','w') as f:\n",
    "        for line in M:\n",
    "            f.write(line+'\\n')\n",
    "    f.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "gpu",
   "language": "python",
   "display_name": "gpu"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}