{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import math\n",
    "import dgl\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from dgl.data import citation_graph as citegrh\n",
    "from dgl.data import CoraBinary\n",
    "from dgl.data import CoraGraphDataset\n",
    "from dgl import DGLGraph\n",
    "import dgl.function as fn\n",
    "import networkx as nx\n",
    "import torch.nn.functional as F\n",
    "from dgl.data import RedditDataset, KarateClubDataset\n",
    "from dgl.nn import GraphConv\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import MSELoss\n",
    "from losses import compute_loss_multiclass\n",
    "\n",
    "\n",
    "class MyModel(th.nn.Module):\n",
    "    def __init__(self, g, dropout, n_features):\n",
    "        '''\n",
    "\n",
    "        :param g:\n",
    "        :param dropout:\n",
    "\n",
    "        c_hat = ReLU(f1*c+f2*(Q C)+b) = (nX1)\n",
    "        Q= nXn\n",
    "        C = nX1\n",
    "        Q*C = nX1\n",
    "        so dimmension of  input is [n,2], output [n,1], Linear layer  [2,1]\n",
    "        '''\n",
    "        super(MyModel, self).__init__()\n",
    "        self.g = g\n",
    "        self.layers = th.nn.ModuleList()\n",
    "        self.layers.append(th.nn.Linear(n_features, 2))\n",
    "        self.layers.append(th.nn.ReLU(inplace=True))\n",
    "        self.dropout = th.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, features):\n",
    "        h = features.float()\n",
    "        for i, layers in enumerate(self.layers):\n",
    "            if i != 0:\n",
    "                h = self.dropout(h)\n",
    "            h = layers(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class ModularityScore(th.nn.Module):\n",
    "    def __init__(self,n_classes,cuda):\n",
    "        super(ModularityScore, self).__init__()\n",
    "        ## define C as parameter\n",
    "        #self.params = th.nn.ParameterList([C])\n",
    "        self.cuda=cuda\n",
    "\n",
    "\n",
    "    def forward(self,C,Q):\n",
    "        # -tf.linalg.trace(tf.matmul(tf.matmul(tf.transpose(C),Q),C))\n",
    "        C=th.sigmoid(C)\n",
    "        Q=Q.float()\n",
    "        if self.cuda:\n",
    "            C=C.cuda()\n",
    "            Q=Q.cuda()\n",
    "        temp = th.matmul(th.matmul(C.t(), Q), C)\n",
    "        loss = -temp.trace()\n",
    "        return loss\n",
    "\n",
    "\n",
    "class GCN(th.nn.Module):\n",
    "    def __init__(self,\n",
    "                 g,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 n_classes,\n",
    "                 n_layers,\n",
    "                 activation,\n",
    "                 dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.g = g\n",
    "        self.layers = th.nn.ModuleList()\n",
    "        # input layer\n",
    "        self.layers.append(GraphConv(in_feats, n_hidden, activation=activation))\n",
    "        # output layer\n",
    "        for i in range(n_layers - 1):\n",
    "            self.layers.append(GraphConv(n_hidden, n_hidden, activation=activation))\n",
    "        # output layer\n",
    "        self.layers.append(GraphConv(n_hidden, n_classes))\n",
    "        self.dropout = th.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, features):\n",
    "        h = features\n",
    "        for i, layers in enumerate(self.layers):\n",
    "            if i != 0:\n",
    "                h = self.dropout(h)\n",
    "            h = layers(self.g, h)\n",
    "        return h\n",
    "\n",
    "\n",
    "def evaluate(model, features, labels, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "        correct = torch.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)\n",
    "\n",
    "\n",
    "def Q2(G1: dgl.DGLGraph):\n",
    "    # calculate matrix Q with diag set to 0\n",
    "    # A=np.array(nx.adjacency_matrix(G1).todense())\n",
    "    G1 = dgl.to_networkx(G1)\n",
    "    A = np.array(nx.adjacency_matrix(G1).todense())\n",
    "    T = A.sum(axis=(0, 1))\n",
    "    Q = A * 0\n",
    "    w_in = A.sum(axis=1)\n",
    "    w_out = w_in.reshape(w_in.shape[0], 1)\n",
    "    K = w_in * w_out / T\n",
    "    Q = (A - K) / T\n",
    "    # set Qii to zero for every i\n",
    "    for i in range(Q.shape[0]):\n",
    "        Q[i][i] = 0\n",
    "    return Q\n",
    "\n",
    "\n",
    "# a utility function to convert a scipy.coo_matrix to torch.SparseFloat\n",
    "def sparse2th(mat):\n",
    "    value = mat.data\n",
    "    indices = th.LongTensor([mat.row, mat.col])\n",
    "    # tensor = th.FloatTensor(th.from_numpy(value).float())\n",
    "    tensor = th.sparse.FloatTensor(indices, th.from_numpy(value).float(), mat.shape)\n",
    "    return tensor.to_dense()\n",
    "\n",
    "\n",
    "# network visualization utility function\n",
    "def visualize(labels, g):\n",
    "    pos = nx.spring_layout(g, seed=1)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.axis('off')\n",
    "    nx.draw_networkx(g, pos=pos, node_size=50, cmap=plt.get_cmap('coolwarm'),\n",
    "                     node_color=labels, edge_color='k',\n",
    "                     arrows=False, width=0.5, style='dotted', with_labels=False)\n",
    "\n",
    "def load_cora_binary():\n",
    "    data = CoraBinary()\n",
    "    g,features,labels=data[1]\n",
    "    n_edges=g.number_of_edges()\n",
    "    features=sparse2th(features)\n",
    "    labels=th.LongTensor(labels)\n",
    "    in_feats=features.shape[1]\n",
    "    n_classes=2\n",
    "    print(th.max(features))\n",
    "\n",
    "    return g,features,n_classes,in_feats,n_edges,labels\n",
    "\n",
    "def load_kara():\n",
    "    data =KarateClubDataset()\n",
    "    n_classes = data.num_classes\n",
    "    g = data[0]\n",
    "    n_edges=g.number_of_edges()\n",
    "    n=len(g.ndata['label'])\n",
    "    labels=g.ndata['label']\n",
    "    #construct features, train,val,test masks\n",
    "    g.ndata['feat']= th.eye(n)\n",
    "    in_feats=g.ndata['feat'].shape[1]\n",
    "    features=torch.FloatTensor(g.ndata['feat'])\n",
    "\n",
    "    return g,features,n_classes,in_feats,labels,n\n",
    "\n",
    "def load_cora():\n",
    "\n",
    "    data = citegrh.load_cora()\n",
    "    features = torch.FloatTensor(data.features)\n",
    "    labels = torch.LongTensor(data.labels)\n",
    "    train_mask = torch.BoolTensor(data.train_mask)\n",
    "    val_mask = torch.BoolTensor(data.val_mask)\n",
    "    test_mask = torch.BoolTensor(data.test_mask)\n",
    "    in_feats = features.shape[1]\n",
    "    n_classes = data.num_labels\n",
    "    n_edges = data.graph.number_of_edges()\n",
    "    g = DGLGraph(data.graph)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\benno\\anaconda3\\envs\\gpu\\lib\\site-packages\\dgl\\data\\dgl_dataset.py\", line 165, in _load\n",
      "    self.load()\n",
      "  File \"C:\\Users\\benno\\anaconda3\\envs\\gpu\\lib\\site-packages\\dgl\\data\\citation_graph.py\", line 874, in load\n",
      "    for i in range(len(lables)):\n",
      "NameError: name 'lables' is not defined\n",
      "\n",
      "Loading from cache failed, re-processing.\n",
      "Done saving data into cached files.\n",
      "Done saving data into cached files.\n",
      "tensor(1.)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0365,  0.0025, -0.0423,  ...,  0.0098, -0.0120,  0.0271],\n",
      "        [-0.0301,  0.0112,  0.0384,  ...,  0.0156, -0.0037,  0.0343],\n",
      "        [-0.0097,  0.0104, -0.0056,  ..., -0.0268, -0.0223, -0.0175],\n",
      "        ...,\n",
      "        [-0.0325, -0.0118,  0.0277,  ...,  0.0222, -0.0203,  0.0042],\n",
      "        [-0.0299,  0.0186,  0.0106,  ..., -0.0127,  0.0396, -0.0044],\n",
      "        [ 0.0043,  0.0014,  0.0185,  ...,  0.0044, -0.0303, -0.0187]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2548, -0.2186,  0.0898,  ..., -0.0249,  0.2928, -0.1478],\n",
      "        [ 0.1661, -0.2124,  0.2574,  ..., -0.3028, -0.2720, -0.1319],\n",
      "        [ 0.2459, -0.0640, -0.0437,  ..., -0.2755, -0.2735,  0.2523],\n",
      "        ...,\n",
      "        [ 0.1240,  0.1136,  0.1260,  ...,  0.1500,  0.1476, -0.2249],\n",
      "        [ 0.1700,  0.2155,  0.1890,  ..., -0.2397, -0.1199, -0.0491],\n",
      "        [-0.1301, -0.0257,  0.0845,  ...,  0.0172, -0.0560,  0.1739]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2751, -0.2267],\n",
      "        [ 0.2330, -0.1477],\n",
      "        [-0.1592, -0.1153],\n",
      "        [ 0.1682, -0.0894],\n",
      "        [-0.3286, -0.2911],\n",
      "        [-0.1023,  0.3747],\n",
      "        [ 0.2267, -0.2776],\n",
      "        [ 0.0222,  0.1134],\n",
      "        [-0.1634,  0.4026],\n",
      "        [-0.3130, -0.0392],\n",
      "        [-0.2930, -0.3538],\n",
      "        [-0.4038, -0.1711],\n",
      "        [-0.3436,  0.1000],\n",
      "        [-0.0881, -0.3223],\n",
      "        [ 0.3357,  0.1517],\n",
      "        [-0.4080, -0.1507],\n",
      "        [-0.0367,  0.2407],\n",
      "        [ 0.1473, -0.1638],\n",
      "        [-0.2813, -0.2282],\n",
      "        [ 0.0156,  0.0341],\n",
      "        [ 0.3133, -0.0220],\n",
      "        [-0.1378,  0.0107],\n",
      "        [-0.2894,  0.3900],\n",
      "        [-0.2827,  0.0371],\n",
      "        [ 0.3666,  0.2587],\n",
      "        [ 0.2335,  0.3567],\n",
      "        [ 0.0440, -0.3590],\n",
      "        [ 0.3953,  0.1457],\n",
      "        [ 0.0145,  0.0063],\n",
      "        [ 0.2012, -0.4100],\n",
      "        [-0.0400, -0.2154],\n",
      "        [-0.3388,  0.3962]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0.], requires_grad=True)\n",
      "Epoch 0 | Time(s) 0.27706146240234375 | Modularity 0.002689448883756995 | Accuracy 0.5674891146589259 | ETputs(KTEPS) 9.68738119234476\n",
      "Epoch 100 | Time(s) 0.013260407022910543 | Modularity 0.0030248186085373163 | Accuracy 0.5573294629898403 | ETputs(KTEPS) 202.40706000673617\n",
      "Epoch 200 | Time(s) 0.011888224094068233 | Modularity 0.003288070671260357 | Accuracy 0.548621190130624 | ETputs(KTEPS) 225.76963377896053\n",
      "Epoch 300 | Time(s) 0.011211818238825498 | Modularity 0.0034502753987908363 | Accuracy 0.5471698113207547 | ETputs(KTEPS) 239.39025257344562\n",
      "Epoch 400 | Time(s) 0.01097740140044481 | Modularity 0.0037542630452662706 | Accuracy 0.5544267053701016 | ETputs(KTEPS) 244.502309981235\n",
      "Epoch 500 | Time(s) 0.010890526210000653 | Modularity 0.003929161466658115 | Accuracy 0.5515239477503628 | ETputs(KTEPS) 246.4527377506618\n",
      "Epoch 600 | Time(s) 0.010815949289255253 | Modularity 0.003992602229118347 | Accuracy 0.5442670537010159 | ETputs(KTEPS) 248.15205103322097\n",
      "Epoch 700 | Time(s) 0.010833988176092782 | Modularity 0.004199954681098461 | Accuracy 0.5457184325108854 | ETputs(KTEPS) 247.73887107637307\n",
      "Epoch 800 | Time(s) 0.01069640904925438 | Modularity 0.004161115735769272 | Accuracy 0.5457184325108854 | ETputs(KTEPS) 250.92533275801517\n",
      "Epoch 900 | Time(s) 0.010589381830806075 | Modularity 0.004326575435698032 | Accuracy 0.5399129172714079 | ETputs(KTEPS) 253.4614430647734\n",
      "Epoch 1000 | Time(s) 0.010558697846266892 | Modularity 0.004336177371442318 | Accuracy 0.5428156748911466 | ETputs(KTEPS) 254.19801182670915\n",
      "Epoch 1100 | Time(s) 0.010534498282284439 | Modularity 0.0046870033256709576 | Accuracy 0.548621190130624 | ETputs(KTEPS) 254.78194861103213\n",
      "Epoch 1200 | Time(s) 0.010541815047061613 | Modularity 0.004662917926907539 | Accuracy 0.5500725689404935 | ETputs(KTEPS) 254.60511192976475\n",
      "Epoch 1300 | Time(s) 0.010556477935931757 | Modularity 0.004627277608960867 | Accuracy 0.5500725689404935 | ETputs(KTEPS) 254.25146685186525\n",
      "Epoch 1400 | Time(s) 0.010521179263205463 | Modularity 0.004778270609676838 | Accuracy 0.5529753265602322 | ETputs(KTEPS) 255.10448333357948\n",
      "Epoch 1500 | Time(s) 0.010473943090216467 | Modularity 0.0046508898958563805 | Accuracy 0.555878084179971 | ETputs(KTEPS) 256.25497263844017\n",
      "Epoch 1600 | Time(s) 0.010445717049717232 | Modularity 0.00464831106364727 | Accuracy 0.5602322206095791 | ETputs(KTEPS) 256.9474155986885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function NDArrayBase.__del__ at 0x000002534C6175E8>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\benno\\anaconda3\\envs\\gpu\\lib\\site-packages\\dgl\\_ffi\\_ctypes\\ndarray.py\", line 70, in __del__\n",
      "    check_call(_LIB.DGLArrayFree(self.handle))\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1700 | Time(s) 0.010452570738615814 | Modularity 0.005496286787092686 | Accuracy 0.5544267053701016 | ETputs(KTEPS) 256.7789366958573\n",
      "Epoch 1800 | Time(s) 0.010459227935265727 | Modularity 0.004976280964910984 | Accuracy 0.5602322206095791 | ETputs(KTEPS) 256.61549940510116\n",
      "Epoch 1900 | Time(s) 0.010445708290393072 | Modularity 0.005169284995645285 | Accuracy 0.5602322206095791 | ETputs(KTEPS) 256.9476310638004\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-2-b3a5916f8d20>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     82\u001B[0m         \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mloss_fcn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mC_hat\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mQ\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     83\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 84\u001B[1;33m         \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     85\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     86\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\gpu\\lib\\site-packages\\torch\\tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    243\u001B[0m                 \u001B[0mcreate_graph\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    244\u001B[0m                 inputs=inputs)\n\u001B[1;32m--> 245\u001B[1;33m         \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    246\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    247\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\gpu\\lib\\site-packages\\torch\\autograd\\__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    145\u001B[0m     Variable._execution_engine.run_backward(\n\u001B[0;32m    146\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 147\u001B[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001B[0m\u001B[0;32m    148\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    149\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dropout = 0.5\n",
    "    gpu = 0\n",
    "    lr = 5e-2\n",
    "    n_epochs = 20000\n",
    "    n_hidden =32  # hidden node number for each layer\n",
    "    n_layers = 2  # number of layer\n",
    "    weight_decay = 5e-4  # weight decay not used here\n",
    "    self_loop = True  # check cycle in the network\n",
    "\n",
    "    # load cora_binary, train_masks,val_masks,test_masks are used for future accuracy comparement with supervised algorithm\n",
    "    g, features, n_classes, in_feats, n_edges,labels = load_cora_binary()\n",
    "    n = len(labels)\n",
    "    train_mask = [True] * n\n",
    "    train_mask=th.BoolTensor(train_mask)\n",
    "    val_mask = train_mask\n",
    "    test_mask = train_mask\n",
    "\n",
    "\n",
    "    #calculate matrix Q, initial community attachment C (with overlap)\n",
    "    Q = Q2(g)\n",
    "    # NOT OVERLAPING CASE\n",
    "    C_init = Q[0:2] * 0\n",
    "    C_init[0] = np.random.randint(2, size=(1, Q.shape[0]))\n",
    "    C_init[1] = 1 - C_init[0]\n",
    "    C = th.tensor(data=C_init.T, requires_grad=True)\n",
    "    C=C.float()\n",
    "    Q = th.from_numpy(Q)\n",
    "    Q=Q.float()\n",
    "    Q_C = th.matmul(Q,C)\n",
    "\n",
    "    if gpu < 0:\n",
    "        cuda = False\n",
    "    else:\n",
    "        cuda = True\n",
    "        torch.cuda.set_device(gpu)\n",
    "        features = features.cuda()\n",
    "        labels = labels.cuda()\n",
    "        g=g.to('cuda:0')\n",
    "        train_mask = train_mask.cuda()\n",
    "        val_mask = val_mask.cuda()\n",
    "        test_mask = test_mask.cuda()\n",
    "\n",
    "    degs = g.in_degrees().float()\n",
    "\n",
    "    model = GCN(g,\n",
    "            in_feats,\n",
    "            n_hidden,\n",
    "            n_classes,\n",
    "            n_layers,\n",
    "            F.relu,\n",
    "            dropout)\n",
    "\n",
    "    for p in model.parameters():\n",
    "        print(p)\n",
    "\n",
    "\n",
    "\n",
    "    # use Modularity score as object function, calculated by M=(C^T)QC\n",
    "    loss_fcn =ModularityScore(n_classes,cuda)\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    for p in loss_fcn.parameters():\n",
    "        print(p)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(),\n",
    "                                  lr=lr)\n",
    "\n",
    "    # train and evaluate (with modularity score and labels)\n",
    "    dur = []\n",
    "    M=[]\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        t0 = time.time()\n",
    "        C_hat = model(features)\n",
    "        loss = loss_fcn(C_hat,Q)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        dur.append(time.time() - t0)\n",
    "        if epoch % 100 == 0:\n",
    "            #record modularity\n",
    "            M.append(str(-loss.item()))\n",
    "            acc_1 = evaluate(model, features, labels, val_mask)\n",
    "            acc_2 = evaluate(model, features, 1 - labels, val_mask)\n",
    "            acc = max(acc_1, acc_2)\n",
    "            print(\"Epoch {} | Time(s) {} | Modularity {} | Accuracy {} | \"\n",
    "                  \"ETputs(KTEPS) {}\".format(epoch, np.mean(dur), -loss,\n",
    "                                                acc, n_edges / np.mean(dur) / 1000))\n",
    "\n",
    "    with open('modularity_history.txt','w') as f:\n",
    "        for line in M:\n",
    "            f.write(line+'\\n')\n",
    "    f.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "gpu",
   "language": "python",
   "display_name": "gpu"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}